# Multilayer-perceptron-from-scratch-using-python 
Multilayer perceptron from scratch for binary classification, I implement 2 activation functions: ReLu and sigmoid, moreover mean square error was implemented as a loss function, which in turn was optimized by gradient descent computed with backpropagation.

Code example used a dataset that was created with "sklearn.datasets.make_circles", furthermore for code example was used a neural net with 2 layers with a learning rate of 0.1, for output layer I used sigmoid function, and for hidden layer Relu.

Dataset distribution can be seen on the below image, moreover the decision boundary of multilayer perceptron can be observed too.

<img width="777" alt="Captura de pantalla 2024-05-03 a la(s) 12 26 40" src="https://github.com/aleperalesg/Multilayer-perceptron-from-scratch-/assets/120703609/53517d5c-6675-4d6d-bdb7-8abe2a88d9b2">
