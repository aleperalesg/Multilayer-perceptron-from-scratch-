# Multilayer-perceptron-from-scratch-using-python 
Multilayer perceptron from scratch for binary classification, I implement 2 activation functions: ReLu and sigmoid, moreover cross-entropy was implemented as a loss function, which in turn was optimized by gradient descent computed with backpropagation.

Code example used a dataset that was created with "sklearn.datasets.make_moons", furthermore for code example was used a neural net with 6 layers with a learning rate of 0.01, for the output layer I used sigmoid function and for the hidden layers Relu and sigmoid.

Dataset distribution can be seen on the below image, moreover the decision boundary of multilayer perceptron can be observed too.
