# Multilayer-perceptron-from-scratch-using-python 
Multilayer perceptron from scratch for binary classification, I implement 2 activation functions: ReLu and sigmoid, moreover cross-entropy was implemented as a loss function, which in turn was optimized by gradient descent computed with backpropagation.

Code example used a dataset that was created with "sklearn.datasets.make_circles", furthermore for code example was used a neural net with 2 layers with a learning rate of 0.01, for the output layer I used sigmoid function and for the hidden layer Relu.

Dataset distribution can be seen on the below image, moreover the decision boundary of multilayer perceptron can be observed too.

![Figure_1](https://github.com/user-attachments/assets/f524980a-b538-4a5d-b746-36555fca0ee6)
